{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c19da0b9",
   "metadata": {},
   "source": [
    "# SMS Spam Detection\n",
    "This notebook includes complete code to classify SMS messages using Machine Learning and Deep Learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "982a0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('../data/SMSSpamCollection.csv', encoding='latin-1')[['label', 'text']]\n",
    "df['label_num'] = df.label.map({'ham':0, 'spam':1})\n",
    "\n",
    "X = df['text']\n",
    "y = df['label_num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f49b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       965\n",
      "           1       0.99      0.84      0.91       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "\n",
      "Results for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       965\n",
      "           1       0.98      0.80      0.88       150\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.90      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 3860\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5646\n",
      "[LightGBM] [Info] Number of data points in the train set: 4457, number of used features: 320\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.133947 -> initscore=-1.866505\n",
      "[LightGBM] [Info] Start training from score -1.866505\n",
      "\n",
      "Results for LightGBM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       965\n",
      "           1       0.95      0.86      0.90       150\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.96      0.93      0.94      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models_ml = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_ml.items():\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    preds = model.predict(X_test_vec)\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b799c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaineb\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8808 - loss: 0.3372 - val_accuracy: 0.9574 - val_loss: 0.1740\n",
      "Epoch 2/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9773 - loss: 0.1260 - val_accuracy: 0.9709 - val_loss: 0.1462\n",
      "Epoch 3/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9882 - loss: 0.1034 - val_accuracy: 0.9686 - val_loss: 0.1616\n",
      "\n",
      "ANN Evaluation:\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9792 - loss: 0.1220\n",
      "Epoch 1/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.8608 - loss: 0.3960 - val_accuracy: 0.9641 - val_loss: 0.1203\n",
      "Epoch 2/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.9833 - loss: 0.0767 - val_accuracy: 0.9776 - val_loss: 0.0727\n",
      "Epoch 3/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.9914 - loss: 0.0362 - val_accuracy: 0.9776 - val_loss: 0.0839\n",
      "\n",
      "LSTM Evaluation:\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9854 - loss: 0.0556\n",
      "Epoch 1/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.8434 - loss: 0.4325 - val_accuracy: 0.9507 - val_loss: 0.1786\n",
      "Epoch 2/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9750 - loss: 0.0956 - val_accuracy: 0.9709 - val_loss: 0.0754\n",
      "Epoch 3/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9904 - loss: 0.0325 - val_accuracy: 0.9776 - val_loss: 0.0651\n",
      "\n",
      "CNN Evaluation:\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9855 - loss: 0.0611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0654425248503685, 0.9820627570152283]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep Learning Models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)\n",
    "\n",
    "# ANN Model\n",
    "model_ann = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_ann.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ann.fit(X_train_seq, y_train, epochs=3, validation_split=0.1)\n",
    "print(\"\\nANN Evaluation:\")\n",
    "model_ann.evaluate(X_test_seq, y_test)\n",
    "\n",
    "# LSTM Model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train_seq, y_train, epochs=3, validation_split=0.1)\n",
    "print(\"\\nLSTM Evaluation:\")\n",
    "model_lstm.evaluate(X_test_seq, y_test)\n",
    "\n",
    "# CNN Model\n",
    "model_cnn = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    Conv1D(32, 3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.fit(X_train_seq, y_train, epochs=3, validation_split=0.1)\n",
    "print(\"\\nCNN Evaluation:\")\n",
    "model_cnn.evaluate(X_test_seq, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d080d44",
   "metadata": {},
   "source": [
    "#Test on spam_texts.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f3bee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text predicted_label\n",
      "0   CREDITED: Rs.75 wallet money. Use it to order ...             ham\n",
      "1   Shoe styles paired with minimal looks, perfect...             ham\n",
      "2   Kejani Cleaning Services offers comprehensive,...             ham\n",
      "3   Carrefour Fridays month is on!! Crazy deals ev...             ham\n",
      "4   Keep up with MTN Broadband! Visit https://apps...             ham\n",
      "5   Get clientele HELP Cover today. Debi check and...             ham\n",
      "6   Do you like your friend's signature? Reply wit...             ham\n",
      "7   Get 2.5GB + 100 Telkom Mins +2 Bob/ Min to oth...             ham\n",
      "8   Enjoy more talktime when you recharge your Air...             ham\n",
      "9   25% Discount - Get Ultra 50GB from MyTelenor A...             ham\n",
      "10  Discount of Rs 100 is now YOURS! Get 126 Chann...             ham\n",
      "11  Ride & save with 25% off 5 GO or GO Awfar ride...             ham\n",
      "12  Study in UK, USA, Canada, Australia, Malaysia,...            spam\n",
      "13  Ramzan Mubarak! Pakistan Day Gala ends 26th Ma...            spam\n",
      "14  Get a SPECIAL DATA DEAL of 2GB for sh. 100 val...             ham\n",
      "15  JAZA POCHI SAA HII!  SHINDA THAO TANO EVERY HO...             ham\n",
      "16  RAILWAY FURNISHERS*Don't Laybuy It - Railway I...             ham\n",
      "17  Unbelievable Offers!! Dial *444# Option 1:Data...             ham\n",
      "18  Elevate your summer style with @signedblake's ...             ham\n",
      "19  Dear Customer, Sell your car at a great price ...             ham\n",
      "20  Hey little one! Exciting news! Mama and baby a...            spam\n",
      "21  Amazing DATA deals on your Pulse Plan today! D...             ham\n",
      "22  Special offer just for you! Get 1GB @15 bob va...             ham\n",
      "23  NEW ARRIVAL - JUNE 23RD  Dresses @ 300; Kondel...             ham\n",
      "24  Coureen, did you know that saving on Timiza in...             ham\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "test_df = pd.read_csv('../data/spam_texts.csv', encoding='latin-1')\n",
    "\n",
    "# Vectorize the text using the same TF-IDF vectorizer used in training\n",
    "test_texts = test_df['text']\n",
    "test_vec = vectorizer.transform(test_texts)\n",
    "\n",
    "# Predict using the best ML model (e.g., Random Forest)\n",
    "test_preds_rf = models_ml['Random Forest'].predict(test_vec)\n",
    "\n",
    "# Add prediction to the dataframe\n",
    "test_df['predicted_label'] = test_preds_rf\n",
    "test_df['predicted_label'] = test_df['predicted_label'].map({0: 'ham', 1: 'spam'})\n",
    "\n",
    "# Show predictions\n",
    "print(test_df[['text', 'predicted_label']])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
