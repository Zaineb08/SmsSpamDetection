{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a9daf0",
   "metadata": {},
   "source": [
    "# SMS Spam Detection Project\n",
    "This notebook presents a complete pipeline to classify SMS messages as spam or ham using both Machine Learning and Deep Learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e0ae2c",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview & Preprocessing\n",
    "We begin by loading the labeled SMS Spam Collection dataset and converting it into a format suitable for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19da0b9",
   "metadata": {},
   "source": [
    "# SMS Spam Detection\n",
    "This notebook includes complete code to classify SMS messages using Machine Learning and Deep Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2481b",
   "metadata": {},
   "source": [
    "## 2. Machine Learning Models\n",
    "We apply traditional machine learning models (Random Forest, XGBoost, LightGBM) using TF-IDF vectorized text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982a0255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('../data/SMSSpamCollection.csv', encoding='latin-1')[['label', 'text']]\n",
    "df['label_num'] = df.label.map({'ham':0, 'spam':1})\n",
    "\n",
    "X = df['text']\n",
    "y = df['label_num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d86b0",
   "metadata": {},
   "source": [
    "## 3. Deep Learning Models\n",
    "We implement deep learning models (ANN, LSTM, CNN) to capture sequential features of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f49b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       965\n",
      "           1       0.99      0.84      0.91       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "\n",
      "Results for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       965\n",
      "           1       0.98      0.80      0.88       150\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.98      0.90      0.93      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 3860\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5646\n",
      "[LightGBM] [Info] Number of data points in the train set: 4457, number of used features: 320\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.133947 -> initscore=-1.866505\n",
      "[LightGBM] [Info] Start training from score -1.866505\n",
      "\n",
      "Results for LightGBM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       965\n",
      "           1       0.95      0.86      0.90       150\n",
      "\n",
      "    accuracy                           0.97      1115\n",
      "   macro avg       0.96      0.93      0.94      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models_ml = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_ml.items():\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    preds = model.predict(X_test_vec)\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b799c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zaineb\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8373 - loss: 0.3625 - val_accuracy: 0.9552 - val_loss: 0.1722\n",
      "Epoch 2/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9776 - loss: 0.1288 - val_accuracy: 0.9709 - val_loss: 0.1439\n",
      "Epoch 3/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9905 - loss: 0.1035 - val_accuracy: 0.9709 - val_loss: 0.1435\n",
      "\n",
      "ANN Evaluation:\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9837 - loss: 0.1180\n",
      "Epoch 1/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 41ms/step - accuracy: 0.8460 - loss: 0.4084 - val_accuracy: 0.9641 - val_loss: 0.1343\n",
      "Epoch 2/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.9841 - loss: 0.0676 - val_accuracy: 0.9709 - val_loss: 0.0746\n",
      "Epoch 3/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 61ms/step - accuracy: 0.9891 - loss: 0.0457 - val_accuracy: 0.9798 - val_loss: 0.0523\n",
      "\n",
      "LSTM Evaluation:\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9801 - loss: 0.0648\n",
      "Epoch 1/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.8530 - loss: 0.4258 - val_accuracy: 0.9552 - val_loss: 0.1863\n",
      "Epoch 2/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9700 - loss: 0.1027 - val_accuracy: 0.9686 - val_loss: 0.0757\n",
      "Epoch 3/3\n",
      "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9908 - loss: 0.0316 - val_accuracy: 0.9776 - val_loss: 0.0644\n",
      "\n",
      "CNN Evaluation:\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9845 - loss: 0.0617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06402628123760223, 0.9811659455299377]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep Learning Models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)\n",
    "\n",
    "# ANN Model\n",
    "model_ann = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_ann.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ann.fit(X_train_seq, y_train, epochs=3, validation_split=0.1)\n",
    "print(\"\\nANN Evaluation:\")\n",
    "model_ann.evaluate(X_test_seq, y_test)\n",
    "\n",
    "# LSTM Model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train_seq, y_train, epochs=3, validation_split=0.1)\n",
    "print(\"\\nLSTM Evaluation:\")\n",
    "model_lstm.evaluate(X_test_seq, y_test)\n",
    "\n",
    "# CNN Model\n",
    "model_cnn = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    Conv1D(32, 3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.fit(X_train_seq, y_train, epochs=3, validation_split=0.1)\n",
    "print(\"\\nCNN Evaluation:\")\n",
    "model_cnn.evaluate(X_test_seq, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69559a",
   "metadata": {},
   "source": [
    "## 4. Final Testing on Unlabeled Dataset\n",
    "We apply all trained models to an external unlabeled dataset provided for evaluation and compare their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c3e9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
      "                                                 text pred_rf pred_xgb  \\\n",
      "0   CREDITED: Rs.75 wallet money. Use it to order ...     ham      ham   \n",
      "1   Shoe styles paired with minimal looks, perfect...     ham      ham   \n",
      "2   Kejani Cleaning Services offers comprehensive,...     ham      ham   \n",
      "3   Carrefour Fridays month is on!! Crazy deals ev...     ham     spam   \n",
      "4   Keep up with MTN Broadband! Visit https://apps...     ham      ham   \n",
      "5   Get clientele HELP Cover today. Debi check and...     ham      ham   \n",
      "6   Do you like your friend's signature? Reply wit...     ham      ham   \n",
      "7   Get 2.5GB + 100 Telkom Mins +2 Bob/ Min to oth...     ham      ham   \n",
      "8   Enjoy more talktime when you recharge your Air...     ham      ham   \n",
      "9   25% Discount - Get Ultra 50GB from MyTelenor A...     ham      ham   \n",
      "10  Discount of Rs 100 is now YOURS! Get 126 Chann...     ham      ham   \n",
      "11  Ride & save with 25% off 5 GO or GO Awfar ride...     ham      ham   \n",
      "12  Study in UK, USA, Canada, Australia, Malaysia,...    spam     spam   \n",
      "13  Ramzan Mubarak! Pakistan Day Gala ends 26th Ma...    spam     spam   \n",
      "14  Get a SPECIAL DATA DEAL of 2GB for sh. 100 val...     ham      ham   \n",
      "15  JAZA POCHI SAA HII!  SHINDA THAO TANO EVERY HO...     ham      ham   \n",
      "16  RAILWAY FURNISHERS*Don't Laybuy It - Railway I...     ham      ham   \n",
      "17  Unbelievable Offers!! Dial *444# Option 1:Data...     ham     spam   \n",
      "18  Elevate your summer style with @signedblake's ...     ham      ham   \n",
      "19  Dear Customer, Sell your car at a great price ...     ham     spam   \n",
      "20  Hey little one! Exciting news! Mama and baby a...    spam     spam   \n",
      "21  Amazing DATA deals on your Pulse Plan today! D...     ham      ham   \n",
      "22  Special offer just for you! Get 1GB @15 bob va...     ham      ham   \n",
      "23  NEW ARRIVAL - JUNE 23RD  Dresses @ 300; Kondel...     ham     spam   \n",
      "24  Coureen, did you know that saving on Timiza in...     ham      ham   \n",
      "\n",
      "   pred_lgb pred_ann pred_lstm pred_cnn  \n",
      "0       ham      ham       ham      ham  \n",
      "1       ham      ham      spam     spam  \n",
      "2       ham      ham      spam      ham  \n",
      "3       ham      ham      spam      ham  \n",
      "4       ham      ham      spam     spam  \n",
      "5       ham      ham      spam     spam  \n",
      "6       ham     spam      spam     spam  \n",
      "7      spam     spam      spam     spam  \n",
      "8       ham     spam      spam     spam  \n",
      "9       ham      ham      spam     spam  \n",
      "10      ham     spam      spam     spam  \n",
      "11      ham      ham      spam      ham  \n",
      "12     spam     spam      spam     spam  \n",
      "13     spam      ham      spam      ham  \n",
      "14      ham      ham      spam      ham  \n",
      "15      ham      ham      spam     spam  \n",
      "16      ham      ham       ham      ham  \n",
      "17     spam      ham      spam     spam  \n",
      "18      ham      ham       ham      ham  \n",
      "19      ham      ham      spam     spam  \n",
      "20     spam      ham      spam     spam  \n",
      "21      ham      ham      spam      ham  \n",
      "22      ham      ham      spam     spam  \n",
      "23     spam     spam      spam     spam  \n",
      "24      ham     spam      spam     spam  \n"
     ]
    }
   ],
   "source": [
    "# Load professor's dataset\n",
    "test_df = pd.read_csv('../data/spam_texts.csv', encoding='latin-1')\n",
    "test_texts = test_df['text']\n",
    "\n",
    "# Vectorize and tokenize\n",
    "test_vec = vectorizer.transform(test_texts)\n",
    "test_seq = pad_sequences(tokenizer.texts_to_sequences(test_texts), maxlen=150)\n",
    "\n",
    "# Predictions from all models\n",
    "test_df['pred_rf'] = models_ml['Random Forest'].predict(test_vec)\n",
    "test_df['pred_xgb'] = models_ml['XGBoost'].predict(test_vec)\n",
    "test_df['pred_lgb'] = models_ml['LightGBM'].predict(test_vec)\n",
    "test_df['pred_ann'] = (model_ann.predict(test_seq) > 0.5).astype(int)\n",
    "test_df['pred_lstm'] = (model_lstm.predict(test_seq) > 0.5).astype(int)\n",
    "test_df['pred_cnn'] = (model_cnn.predict(test_seq) > 0.5).astype(int)\n",
    "\n",
    "# Format predictions to 'ham' or 'spam'\n",
    "for col in ['pred_rf', 'pred_xgb', 'pred_lgb', 'pred_ann', 'pred_lstm', 'pred_cnn']:\n",
    "    test_df[col] = test_df[col].astype(int).map({0: 'ham', 1: 'spam'})\n",
    "\n",
    "# Display comparison\n",
    "comparison = test_df[['text', 'pred_rf', 'pred_xgb', 'pred_lgb', 'pred_ann', 'pred_lstm', 'pred_cnn']]\n",
    "print(comparison)\n",
    "\n",
    "# Optional export\n",
    "comparison.to_csv('../report/final_model_comparison.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
