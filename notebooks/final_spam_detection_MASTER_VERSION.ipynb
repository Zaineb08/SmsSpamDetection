{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1554e3af",
   "metadata": {},
   "source": [
    "# üìä SMS Spam Detection Project\n",
    "This notebook demonstrates a complete pipeline to classify SMS messages as spam or ham using both Machine Learning and Deep Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859cc51c",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Dataset Overview & Preprocessing\n",
    "We begin by loading the labeled SMS Spam Collection dataset and converting it into a format suitable for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19da0b9",
   "metadata": {},
   "source": [
    "# SMS Spam Detection\n",
    "This notebook includes complete code to classify SMS messages using Machine Learning and Deep Learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767eac0",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Machine Learning Models\n",
    "We apply classic machine learning models (Random Forest, XGBoost, LightGBM) using TF-IDF vectorized text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982a0255",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['label'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/spam_texts.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatin-1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_num\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mham\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspam\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m})\n\u001b[0;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['label'] not in index\""
     ]
    }
   ],
   "source": [
    "# Data Loading and Preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df = pd.read_csv('../data/SMSSpamCollection.csv', encoding='latin-1')[['label', 'text']]\n",
    "df['label_num'] = df.label.map({'ham':0, 'spam':1})\n",
    "\n",
    "X = df['text']\n",
    "y = df['label_num']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ef91e5",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Deep Learning Models\n",
    "We implement modern deep learning models (ANN, LSTM, CNN) to capture sequential features of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f49b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "models_ml = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models_ml.items():\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    preds = model.predict(X_test_vec)\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b799c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning Models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)\n",
    "\n",
    "# ANN Model\n",
    "model_ann = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_ann.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ann.fit(X_train_seq, y_train, epochs=3, validation_split=0.1)\n",
    "print(\"\\nANN Evaluation:\")\n",
    "model_ann.evaluate(X_test_seq, y_test)\n",
    "\n",
    "# LSTM Model\n",
    "model_lstm = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    LSTM(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train_seq, y_train, epochs=3, validation_split=0.1)\n",
    "print(\"\\nLSTM Evaluation:\")\n",
    "model_lstm.evaluate(X_test_seq, y_test)\n",
    "\n",
    "# CNN Model\n",
    "model_cnn = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    Conv1D(32, 3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.fit(X_train_seq, y_train, epochs=3, validation_split=0.1)\n",
    "print(\"\\nCNN Evaluation:\")\n",
    "model_cnn.evaluate(X_test_seq, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò Final Testing on Professor's Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load professor's dataset\n",
    "test_df = pd.read_csv('../data/spam_texts.csv', encoding='latin-1')\n",
    "\n",
    "# Vectorize using the same trained TF-IDF vectorizer\n",
    "test_texts = test_df['text']\n",
    "test_vec = vectorizer.transform(test_texts)\n",
    "\n",
    "# Predict using trained Random Forest model\n",
    "test_preds = models_ml['Random Forest'].predict(test_vec)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "test_df['predicted_label'] = test_preds\n",
    "test_df['predicted_label'] = test_df['predicted_label'].map({0: 'ham', 1: 'spam'})\n",
    "\n",
    "# Show results\n",
    "print(test_df[['text', 'predicted_label']])\n",
    "\n",
    "# Optional: export results for use in your report\n",
    "test_df[['text', 'predicted_label']].to_csv('../report/test_predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bdb1a6",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Final Testing on Unlabeled Dataset\n",
    "Here, we apply our best trained model to a separate unlabeled dataset provided for testing and observe the predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
